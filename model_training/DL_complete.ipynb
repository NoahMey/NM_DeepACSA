{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python 3.8.16 used\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#unet collection\n",
    "from keras_unet_collection import models\n",
    "\n",
    "#additional Imports for VGG16\n",
    "import random\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Conv2DTranspose, Concatenate, Input\n",
    "from tensorflow.keras.applications import VGG16 \n",
    "from keras.layers import Input, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define directory where images and masks are located on local disk\n",
    "image_directory = 'D:/UniBas/Bachelorarbeit/Img_masks/DeepACSA_images_RF/insert_images/'\n",
    "mask_directory = 'D:/UniBas/Bachelorarbeit/Img_masks/DeepACSA_masks_RF/insert_masks/'   \n",
    "\n",
    "#define some hyperparameters\n",
    "SIZE = 256\n",
    "num_labels = 1  #Binary classificaion is n_labels= 2, missmatch with literature! Swin and r2 require num_labels = 2, but works with 1 also.\n",
    "batch_size = 2  #keep it smaller than 3\n",
    "epochs = 60\n",
    "num_folds = 5   #define the number of folds (usually 5-10 folds)\n",
    "\n",
    "#define custom functions\n",
    "def IoU(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    union = K.sum(y_true,-1) + K.sum(y_pred,-1) - intersection\n",
    "    iou = (intersection + smooth) / ( union + smooth)\n",
    "    return iou\n",
    "\n",
    "def datasets():\n",
    "    image_dataset = []\n",
    "    mask_dataset = []\n",
    "\n",
    "    #enumerate and resize images/masks\n",
    "    images = os.listdir(image_directory)\n",
    "    for i, image_name in enumerate(images):    #enumerate method adds a counter and returns the enumerate object\n",
    "        if (image_name.split('.')[1] == 'tif'):\n",
    "            #print(image_directory+image_name)\n",
    "            image = cv2.imread(image_directory+image_name, 1)\n",
    "            image = Image.fromarray(image)\n",
    "            image = image.resize((SIZE, SIZE))\n",
    "            image_dataset.append(np.array(image))\n",
    "\n",
    "    masks = os.listdir(mask_directory)\n",
    "    for i, image_name in enumerate(masks):\n",
    "        if (image_name.split('.')[1] == 'tif'):\n",
    "            image = cv2.imread(mask_directory+image_name, 0)\n",
    "            image = Image.fromarray(image)\n",
    "            image = image.resize((SIZE, SIZE))\n",
    "            mask_dataset.append(np.array(image))\n",
    "\n",
    "    #normalize images\n",
    "    image_dataset = np.array(image_dataset)/255\n",
    "    #do not normalize masks, just rescale to 0 to 1. Add RGB-Chanel (3) to mask.\n",
    "    mask_dataset = np.expand_dims((np.array(mask_dataset)),3) /255\n",
    "    return mask_dataset, image_dataset\n",
    "\n",
    "#determine best fold and create csv file\n",
    "def best_fold(searchterm):\n",
    "    fo_path = os.getcwd() \n",
    "    max_val_iou = 0.0\n",
    "    fold = \"\"\n",
    "    for file_name in os.listdir(fo_path):\n",
    "        if searchterm in file_name and file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(fo_path, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(file_path)\n",
    "            if \"val_IoU\" in df.columns:\n",
    "                val_iou = df[\"val_IoU\"].max()\n",
    "            if val_iou > max_val_iou:\n",
    "                max_val_iou = val_iou\n",
    "                fold = file_name\n",
    "\n",
    "    #save the results to a CSV file\n",
    "    results = pd.DataFrame({\"fold\": [fold], \"max_val_iou\": [max_val_iou]})\n",
    "    results.to_csv(f\"{searchterm}results.csv\", index=False)\n",
    "\n",
    "#make sure the to_categorical function runs only once\n",
    "runcheck_categorical = False\n",
    "\n",
    "#define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, random_state= 42 ,shuffle=True)\n",
    "fold_no = 1\n",
    "mask_dataset, image_dataset = datasets()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with unet_2plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure the mask_dataset has the right shape. Only run to.categroical once for Swin and r2 Unet!\n",
    "if runcheck_categorical == True:\n",
    "    mask_dataset, image_dataset = datasets()\n",
    "    \n",
    "architecture = \"unet_2plus\"\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "\n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(f\"B1VL-Kfoldno{fold_no}-{architecture}.h5\", verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f\"B1VL-Kfoldno{fold_no}-{architecture}.csv\", separator=\",\", append=False)]\n",
    "\n",
    "  #define the model architecture\n",
    "  #unet_plus_2d requires a Backbone\n",
    "  model = models.unet_plus_2d((256, 256, 3), filter_num=[64, 128, 256, 512, 1024], \n",
    "                           n_labels=num_labels, \n",
    "                           stack_num_down=2, stack_num_up=2, \n",
    "                           activation=\"ReLU\", \n",
    "                           output_activation=\"Sigmoid\", \n",
    "                           batch_norm=False, pool=False, unpool=False, \n",
    "                           backbone=\"VGG16\", weights=\"imagenet\", \n",
    "                           freeze_backbone=True, freeze_batch_norm=True, \n",
    "                           name=\"unet_plus\")\n",
    "\n",
    "  #compile the model\n",
    "  model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr = 1e-3), \n",
    "              metrics=[\"accuracy\", IoU])\n",
    "\n",
    "  #generate a print\n",
    "  print(\"------------------------------------------------------------------------\")\n",
    "  print(f\"Training for fold {fold_no} ...\")\n",
    "\n",
    "  #fit model to data\n",
    "  Unet_plus_history = model.fit(image_dataset[train], mask_dataset[train], \n",
    "                    verbose=1,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data=(image_dataset[test], mask_dataset[test]), \n",
    "                    shuffle=False,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "  #increase fold number\n",
    "  fold_no += 1\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "#determine best fold\n",
    "best_fold(architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with unet_3plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure the mask_dataset has the right shape. Only run to.categroical once for Swin and r2 Unet!\n",
    "if runcheck_categorical == True:\n",
    "    mask_dataset, image_dataset = datasets()\n",
    "    \n",
    "architecture = \"unet_3plus\"\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "\n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(f'B1VL-Kfoldno{fold_no}-{architecture}.h5', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f'B1VL-Kfoldno{fold_no}-{architecture}.csv', separator=',', append=False)]\n",
    "\n",
    "  # Define the model architecture\n",
    "  # unet_plus_2d require depth >= 2\n",
    "  model = models.unet_3plus_2d((256, 256, 3), n_labels=num_labels, filter_num_down=[64, 128, 256, 512, 1024], \n",
    "                             filter_num_skip='auto', filter_num_aggregate='auto', \n",
    "                             stack_num_down=2, stack_num_up=2, activation='ReLU', output_activation='Sigmoid',\n",
    "                             batch_norm=True, pool=True, unpool=False, deep_supervision=False, name='unet3plus')\n",
    "  # Compile the model\n",
    "  model.compile(loss='binary_crossentropy', optimizer=Adam(lr = 1e-3), \n",
    "              metrics=['accuracy', IoU])\n",
    "\n",
    "  # Generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  # Fit data to model\n",
    "  Unet_plus_history = model.fit(image_dataset[train], mask_dataset[train], \n",
    "                    verbose=1,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data=(image_dataset[test], mask_dataset[test]), \n",
    "                    shuffle=False,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "  # Increase fold number\n",
    "  fold_no += 1\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "#determine best fold\n",
    "best_fold(architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with Trans_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure the mask_dataset has the right shape. Only run to.categroical once for Swin and r2 Unet!\n",
    "if runcheck_categorical == True:\n",
    "    mask_dataset, image_dataset = datasets()\n",
    "\n",
    "architecture = \"Trans_unet\"\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(f'B1VL-Kfoldno{fold_no}-{architecture}.h5', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f'B1VL-Kfoldno{fold_no}-{architecture}.csv', separator=',', append=False)]\n",
    "  \n",
    "  #define the model architecture\n",
    "  model = models.transunet_2d((256, 256, 3), filter_num=[64, 128, 256, 512, 1024], \n",
    "                          n_labels=num_labels, stack_num_down=2, stack_num_up=2, \n",
    "                          embed_dim=768, num_mlp=3072, num_heads=12, num_transformer=12, \n",
    "                          activation='ReLU', mlp_activation='GELU', output_activation='Sigmoid', #output activation from Softmax to Sigmoid\n",
    "                          batch_norm=True, pool=True, unpool=False, name='transunet')\n",
    "                          #batchnorm to true, unpool to false\n",
    "\n",
    "  #compile the model\n",
    "  model.compile(loss='binary_crossentropy', optimizer=Adam(lr = 1e-3), \n",
    "              metrics=['accuracy', IoU])\n",
    "\n",
    "  #generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  #fit model to data\n",
    "  transunet_history = model.fit(image_dataset[train], mask_dataset[train], \n",
    "                    verbose=1,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data=(image_dataset[test], mask_dataset[test]), \n",
    "                    shuffle=False,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "  \n",
    "  #increase fold number\n",
    "  fold_no += 1\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "#determine best fold\n",
    "best_fold(architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with Swin_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure the mask_dataset has the right shape. Only run to.categroical once.\n",
    "if runcheck_categorical == False:\n",
    "    mask_dataset = tf.keras.utils.to_categorical(mask_dataset)\n",
    "    runcheck_categorical = True\n",
    "\n",
    "architecture=\"Swin\"\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "#define callback function\n",
    "    callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(f'B1-Kfoldno{fold_no}-{architecture}.h5', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f'B1-Kfoldno{fold_no}-{architecture}.csv', separator=',', append=False)]\n",
    "        \n",
    "    #define the model architecture\n",
    "    #this model requires depth >= 2\n",
    "    model = models.swin_unet_2d((256, 256, 3), filter_num_begin=64, n_labels=2, #n_labels !=2\n",
    "                                depth=4, stack_num_down=2, stack_num_up=2, \n",
    "                                patch_size=(4, 4), num_heads=[4, 8, 8, 8], window_size=[4, 2, 2, 2], num_mlp=512, \n",
    "                                output_activation='Softmax', shift_window=True, name='swin_unet') #Guess: Shift_window = False\n",
    "\n",
    "    #compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr = 1e-3), \n",
    "                metrics=['accuracy', IoU])\n",
    "\n",
    "    #split dataset into training and validation set\n",
    "    \n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    #fit model to data\n",
    "    resunet_history = model.fit(image_dataset[train], mask_dataset[train], \n",
    "                        verbose=1,\n",
    "                        batch_size = batch_size,\n",
    "                        validation_data=(image_dataset[test], mask_dataset[test]), \n",
    "                        shuffle=False,\n",
    "                        epochs=epochs,\n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "    fold_no += 1\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "#determine best fold\n",
    "best_fold(architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with R2_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure the mask_dataset has the right shape. Only run to.categroical once.\n",
    "if runcheck_categorical == False:\n",
    "    mask_dataset = tf.keras.utils.to_categorical(mask_dataset)\n",
    "    runcheck_categorical = True\n",
    "\n",
    "architecture=\"R2-Unet\"\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "#define callback function\n",
    "    callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(f'B1-Kfoldno{fold_no}-{architecture}.h5', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f'B1-Kfoldno{fold_no}-{architecture}.csv', separator=',', append=False)]\n",
    "        \n",
    "    #define the model architecture\n",
    "    #this model requires depth >= 2\n",
    "    model = models.r2_unet_2d((256, 256, 3), [64, 128, 256, 512], n_labels=2, #n_labels !=2\n",
    "                        stack_num_down=4, stack_num_up=4, recur_num=4,\n",
    "                        activation='ReLU', output_activation='Softmax', \n",
    "                        batch_norm=True, pool=True, unpool=\"bilinear\", name='r2unet')\n",
    "\n",
    "    #compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr = 1e-3), \n",
    "                metrics=['accuracy', IoU])\n",
    "\n",
    "    #split dataset into training and validation set\n",
    "    \n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    #fit model to data\n",
    "    resunet_history = model.fit(image_dataset[train], mask_dataset[train], \n",
    "                        verbose=1,\n",
    "                        batch_size = batch_size,\n",
    "                        validation_data=(image_dataset[test], mask_dataset[test]), \n",
    "                        shuffle=False,\n",
    "                        epochs=epochs,\n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "    fold_no += 1\n",
    "    \n",
    "fold_no = 1\n",
    "\n",
    "#determine best fold\n",
    "best_fold(architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure the mask_dataset has the right shape. Only run to.categroical once for Swin and r2 Unet!\n",
    "if runcheck_categorical == True:\n",
    "    mask_dataset, image_dataset = datasets()\n",
    "\n",
    "def conv_block(inputs, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding = \"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    x = Conv2D(num_filters, 3, padding = \"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def decoder_block(inputs, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs) #32\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def build_vgg16_unet(input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    vgg16 = VGG16(include_top=False, weights=\"imagenet\", input_tensor = inputs)\n",
    "    #vgg16.summary()\n",
    "    \n",
    "    \"\"\" Encoder \"\"\"\n",
    "    \n",
    "    # skip connections\n",
    "    s1 = vgg16.get_layer(\"block1_conv2\").output # 256\n",
    "    s2 = vgg16.get_layer(\"block2_conv2\").output # 128\n",
    "    s3 = vgg16.get_layer(\"block3_conv3\").output # 64\n",
    "    s4 = vgg16.get_layer(\"block4_conv3\").output # 32\n",
    "\n",
    "    \"\"\" Bottleneck/Bridge \"\"\"\n",
    "    \n",
    "    b1 = vgg16.get_layer(\"block5_conv3\").output # 16\n",
    "    \n",
    "    \"\"\" Decoder \"\"\"\n",
    "\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "    \n",
    "    \"\"\" Outputs \"\"\"\n",
    "    outputs = Conv2D(1, (1, 1), padding = \"same\", activation=\"sigmoid\")(d4) #binary segmentation\n",
    "    model = Model(inputs, outputs, name = \"VGG16_U-Net\")\n",
    "    return model\n",
    "    \n",
    "# Convolution block\n",
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size), \\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Create u-net model\n",
    "def get_unet(input_img, n_filters = 64, dropout = 0.1, batchnorm = True):\n",
    "    \"\"\"Function to define the UNET Model\"\"\"\n",
    "    \n",
    "    # Contracting Path\n",
    "    # c is output tensor of conv layers\n",
    "    # p ist output tensor of max pool layers\n",
    "    # u is output tensor of up-sampling (transposed) layers\n",
    "    # Batchnorm standardizes/normalizes the output of each layer where applied in order to avoid huge weights using \n",
    "    # z-scores \n",
    "    \n",
    "    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    p1 = Dropout(dropout)(p1)\n",
    "    \n",
    "    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "    \n",
    "    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "    \n",
    "    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "    \n",
    "    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    # Expansive Path\n",
    "    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# Compute Intersection over union (IoU), a measure of labelling accuracy\n",
    "def dice_score(y_true, y_pred, smooth=1):\n",
    "    # Flatten\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    \n",
    "    return 1 - dice_score(y_true, y_pred)\n",
    "\n",
    "def dice_bce_score(y_true, y_pred, smooth=1):    \n",
    "    \n",
    "    BCE =  K.binary_crossentropy(y_true, y_pred)\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)    \n",
    "    dice_loss = 1 - (2*intersection + smooth) / (K.sum(y_true, -1) + K.sum(y_pred, -1) + smooth)\n",
    "    Dice_BCE = BCE + dice_loss\n",
    "    \n",
    "    return Dice_BCE\n",
    "\n",
    "def focal_loss(y_true, y_pred, alpha=0.8, gamma=2):    \n",
    "      \n",
    "    BCE = K.binary_crossentropy(y_true, y_pred)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "    return focal_loss\n",
    "\n",
    "# Plot sample of model prediction\n",
    "def plot_sample(X, y, preds, binary_preds, ix=None):\n",
    "    if ix is None:\n",
    "        ix = random.randint(0, len(X))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(30, 20))\n",
    "    ax[0].imshow(X[ix, ..., 0], cmap='Greys_r')\n",
    "    \n",
    "    ax[0].set_title('US-image', c=\"white\" )\n",
    "    ax[0].grid(False)\n",
    "\n",
    "    ax[1].imshow(y[ix].squeeze(), cmap='Greys_r')\n",
    "    ax[1].set_title('Aponeurosis', c=\"white\")\n",
    "    ax[1].grid(False)\n",
    "\n",
    "    ax[2].imshow(preds[ix].squeeze(), vmin=0, vmax=1, cmap=\"Greys_r\")\n",
    "    \n",
    "    ax[2].set_title('Apo-Predicted', c=\"white\")\n",
    "    ax[2].grid(False)\n",
    "    \n",
    "    ax[3].imshow(binary_preds[ix].squeeze(), vmin=0, vmax=0.5, cmap=\"Greys_r\")\n",
    "    \n",
    "    ax[3].set_title('Apo-Picture binary', c=\"white\")\n",
    "    ax[3].grid(False)\n",
    "    \n",
    "    plt.savefig(str(ix)+\"Pred_area.tif\")\n",
    "\n",
    "# Save all predictions on disk \n",
    "def save_pred_area(binary_preds): \n",
    "    for i in range(len(binary_preds)): \n",
    "        fig, (ax1)= plt.subplots(1, 1, figsize = (15, 15))\n",
    "        ax1.imshow(binary_preds[i], cmap=\"Greys_r\", interpolation=\"bilinear\")\n",
    "        ax1.set_title(\"Predicted Area\")\n",
    "        plt.savefig(str(i)+\"Pred_area.tif\") # Saves images to directory of notebook\n",
    "\n",
    "# Convolution block\n",
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size), \\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "#compile the model\n",
    "VGG16_UNet = build_vgg16_unet((256,256,3)) #input_shape is (256, 256, 3)\n",
    "model_apo = VGG16_UNet\n",
    "model_apo.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=[\"accuracy\", IoU])\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "architecture = \"VGG16\"\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "\n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(f'B1VL-Kfoldno{fold_no}-{architecture}.h5', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f'B1VL-Kfoldno{fold_no}-{architecture}.csv', separator=',', append=False)\n",
    "  ]\n",
    "\n",
    "  # Generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  # Fit data to model\n",
    "  results = model_apo.fit(image_dataset[train], mask_dataset[train], batch_size=batch_size, epochs=epochs,\n",
    "                       callbacks=callbacks, validation_data=(image_dataset[test], mask_dataset[test]))\n",
    "\n",
    "  # Increase fold number\n",
    "  fold_no = fold_no + 1\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "#determine best fold\n",
    "best_fold(architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with ResUnet (not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure the mask_dataset has the right shape. Only run to.categroical once.\n",
    "if runcheck_categorical == False:\n",
    "    mask_dataset = tf.keras.utils.to_categorical(mask_dataset)\n",
    "    runcheck_categorical = True\n",
    "\n",
    "architecture = \"ResUnet\"\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(f'B1VL-Kfoldno{fold_no}-{architecture}.h5', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f'B1VL-Kfoldno{fold_no}-{architecture}.csv', separator=',', append=False)]\n",
    "\n",
    "  # Define the model architecture\n",
    "  # resunet requires depth >= 2\n",
    "  resunet = models.resunet_a_2d((256, 256, 3), [64, 128, 256, 512], \n",
    "                            dilation_num=[1, 3, 15, 31], \n",
    "                            n_labels=2, aspp_num_down=256, aspp_num_up=128, \n",
    "                            activation='ReLU', output_activation='Softmax', \n",
    "                            batch_norm=True, pool=\"max\", unpool='nearest', name='resunet')\n",
    "\n",
    "  # Compile the model\n",
    "  resunet.compile(loss='binary_crossentropy', optimizer=Adam(lr = 1e-3), \n",
    "              metrics=['accuracy', IoU])\n",
    "\n",
    "  # Generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  # Fit data to model\n",
    "  resunet_history = resunet.fit(image_dataset[train], mask_dataset[train], \n",
    "                    verbose=1,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data=(image_dataset[test], mask_dataset[test]), \n",
    "                    shuffle=False,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "  # Increase fold number\n",
    "  fold_no += 1\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "#determine best fold\n",
    "best_fold(architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide average scores for K-fold cross validation (outdated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]} - IoU: {IoU_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print(f'> IoU: {np.mean(IoU_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepACSA5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
