{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import cv2\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.optimizers import Adam\n",
    "from keras import utils\n",
    "\n",
    "#unet collection\n",
    "from keras_unet_collection import models\n",
    "#import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#Python 3.8.16 used\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define directory where images and masks are located on local disk\n",
    "image_directory = 'D:/UniBas/Bachelorarbeit/Img_masks/DeepACSA_images_RF/insert_images/' ##VGG16 needs seperate induction of path\n",
    "mask_directory = 'D:/UniBas/Bachelorarbeit/Img_masks/DeepACSA_masks_RF/insert_masks/'   ##VGG16 need seperate induction of path\n",
    "\n",
    "#define the properties and empty list for resized images and masks\n",
    "SIZE = 256\n",
    "image_dataset = []\n",
    "mask_dataset = []\n",
    "\n",
    "#define custom function\n",
    "def IoU(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    union = K.sum(y_true,-1) + K.sum(y_pred,-1) - intersection\n",
    "    iou = (intersection + smooth) / ( union + smooth)\n",
    "    return iou\n",
    "\n",
    "#enumerate and resize images/masks\n",
    "images = os.listdir(image_directory)\n",
    "for i, image_name in enumerate(images):    #enumerate method adds a counter and returns the enumerate object\n",
    "    if (image_name.split('.')[1] == 'tif'):\n",
    "        #print(image_directory+image_name)\n",
    "        image = cv2.imread(image_directory+image_name, 1)\n",
    "        image = Image.fromarray(image)\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        image_dataset.append(np.array(image))\n",
    "\n",
    "masks = os.listdir(mask_directory)\n",
    "for i, image_name in enumerate(masks):\n",
    "    if (image_name.split('.')[1] == 'tif'):\n",
    "        image = cv2.imread(mask_directory+image_name, 0)\n",
    "        image = Image.fromarray(image)\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        mask_dataset.append(np.array(image))\n",
    "\n",
    "#define some hyperparameters\n",
    "num_labels = 1  #Binary classificaion\n",
    "batch_size = 2  #keep it smaller than 3\n",
    "epochs = 60\n",
    "num_folds = 5   #define the number of folds (usually 5-10 folds)\n",
    "\n",
    "#normalize images\n",
    "image_dataset = np.array(image_dataset)/255\n",
    "#do not normalize masks, just rescale to 0 to 1. Add RGB-Chanel (3) to mask.\n",
    "mask_dataset = np.expand_dims((np.array(mask_dataset)),3) /255\n",
    "mask_dataset = tf.keras.utils.to_categorical(mask_dataset)\n",
    "\n",
    "#define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, random_state= 42 ,shuffle=True)\n",
    "\n",
    "#define per-fold score containers \n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "IoU_per_fold = []\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "#determine best fold and create csv file\n",
    "def best_fold(searchterm):\n",
    "    fo_path = os.getcwd() \n",
    "    max_val_iou = 0.0\n",
    "    fold = \"\"\n",
    "    for file_name in os.listdir(fo_path):\n",
    "        if searchterm in file_name and file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(fo_path, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(file_path)\n",
    "            if \"val_IoU\" in df.columns:\n",
    "                val_iou = df[\"val_IoU\"].max()\n",
    "            if val_iou > max_val_iou:\n",
    "                max_val_iou = val_iou\n",
    "                fold = file_name\n",
    "\n",
    "    #save the results to a CSV file\n",
    "    results = pd.DataFrame({\"fold\": [fold], \"max_val_iou\": [max_val_iou]})\n",
    "    results.to_csv(f\"{searchterm}results.csv\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with unet_2plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = \"unet_2plus\"\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "\n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(f\"B1VL-Kfoldno{fold_no}-{architecture}.h5\", verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f\"B1VL-Kfoldno{fold_no}-{architecture}.csv\", separator=\",\", append=False)]\n",
    "\n",
    "  #define the model architecture\n",
    "  #unet_plus_2d requires a Backbone\n",
    "  model = models.unet_plus_2d((256, 256, 3), filter_num=[64, 128, 256, 512, 1024], \n",
    "                           n_labels=num_labels, \n",
    "                           stack_num_down=2, stack_num_up=2, \n",
    "                           activation=\"ReLU\", \n",
    "                           output_activation=\"Sigmoid\", \n",
    "                           batch_norm=False, pool=False, unpool=False, \n",
    "                           backbone=\"VGG16\", weights=\"imagenet\", \n",
    "                           freeze_backbone=True, freeze_batch_norm=True, \n",
    "                           name=\"unet_plus\")\n",
    "\n",
    "  #compile the model\n",
    "  model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr = 1e-3), \n",
    "              metrics=[\"accuracy\", IoU])\n",
    "\n",
    "  #generate a print\n",
    "  print(\"------------------------------------------------------------------------\")\n",
    "  print(f\"Training for fold {fold_no} ...\")\n",
    "\n",
    "  #fit model to data\n",
    "  Unet_plus_history = model.fit(image_dataset[train], mask_dataset[train], \n",
    "                    verbose=1,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data=(image_dataset[test], mask_dataset[test]), \n",
    "                    shuffle=False,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "  \n",
    "  #append evaluation values for every fold to a list\n",
    "  acc_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[1])\n",
    "  loss_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[0])\n",
    "  IoU_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[2])\n",
    "\n",
    "  #increase fold number\n",
    "  fold_no += 1\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "#determine best fold\n",
    "best_fold(architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with unet_3plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = \"unet_3plus\"\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "\n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(f'B1VL-Kfoldno{fold_no}-{architecture}.h5', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f'B1VL-Kfoldno{fold_no}-{architecture}.csv', separator=',', append=False)]\n",
    "\n",
    "  # Define the model architecture\n",
    "  # unet_plus_2d require depth >= 2\n",
    "  model = models.unet_3plus_2d((256, 256, 3), n_labels=num_labels, filter_num_down=[64, 128, 256, 512, 1024], \n",
    "                             filter_num_skip='auto', filter_num_aggregate='auto', \n",
    "                             stack_num_down=2, stack_num_up=2, activation='ReLU', output_activation='Sigmoid',\n",
    "                             batch_norm=True, pool=True, unpool=False, deep_supervision=False, name='unet3plus')\n",
    "  # Compile the model\n",
    "  model.compile(loss='binary_crossentropy', optimizer=Adam(lr = 1e-3), \n",
    "              metrics=['accuracy', IoU])\n",
    "\n",
    "  # Generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  # Fit data to model\n",
    "  Unet_plus_history = model.fit(image_dataset[train], mask_dataset[train], \n",
    "                    verbose=1,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data=(image_dataset[test], mask_dataset[test]), \n",
    "                    shuffle=False,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "  \n",
    "  #append evaluation values for every fold to a list\n",
    "  acc_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[1])\n",
    "  loss_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[0])\n",
    "  IoU_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[2])\n",
    "\n",
    "  # Increase fold number\n",
    "  fold_no += 1\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "#determine best fold\n",
    "best_fold(architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with Trans_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = \"Trans_unet\"\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(f'B1VL-Kfoldno{fold_no}-{architecture}.h5', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f'B1VL-Kfoldno{fold_no}-{architecture}.csv', separator=',', append=False)]\n",
    "  \n",
    "  #define the model architecture\n",
    "  model = models.transunet_2d((256, 256, 3), filter_num=[64, 128, 256, 512, 1024], \n",
    "                          n_labels=num_labels, stack_num_down=2, stack_num_up=2, \n",
    "                          embed_dim=768, num_mlp=3072, num_heads=12, num_transformer=12, \n",
    "                          activation='ReLU', mlp_activation='GELU', output_activation='Sigmoid', #output activation from Softmax to Sigmoid\n",
    "                          batch_norm=True, pool=True, unpool=False, name='transunet')\n",
    "                          #batchnorm to true, unpool to false\n",
    "\n",
    "  #compile the model\n",
    "  model.compile(loss='binary_crossentropy', optimizer=Adam(lr = 1e-3), \n",
    "              metrics=['accuracy', IoU])\n",
    "\n",
    "  #generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  #fit model to data\n",
    "  transunet_history = model.fit(image_dataset[train], mask_dataset[train], \n",
    "                    verbose=1,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data=(image_dataset[test], mask_dataset[test]), \n",
    "                    shuffle=False,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "  #append evaluation values for every fold to a list\n",
    "  acc_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[1])\n",
    "  loss_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[0])\n",
    "  IoU_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[2])\n",
    "\n",
    "  #increase fold number\n",
    "  fold_no += 1\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "#determine best fold\n",
    "best_fold(architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with Swin_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/60\n",
      "  7/205 [>.............................] - ETA: 2:58 - loss: 7.6246 - accuracy: 0.5000 - IoU: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining for fold \u001b[39m\u001b[39m{\u001b[39;00mfold_no\u001b[39m}\u001b[39;00m\u001b[39m ...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[39m#fit model on data\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m swin_unet_history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(image_dataset[train], mask_dataset[train], \n\u001b[0;32m     25\u001b[0m                   verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     26\u001b[0m                   batch_size \u001b[39m=\u001b[39;49m batch_size,\n\u001b[0;32m     27\u001b[0m                   validation_data\u001b[39m=\u001b[39;49m(image_dataset[test], mask_dataset[test]), \n\u001b[0;32m     28\u001b[0m                   shuffle\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     29\u001b[0m                   epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     30\u001b[0m                   callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[0;32m     32\u001b[0m \u001b[39m#append evaluation values for every fold to a list\u001b[39;00m\n\u001b[0;32m     33\u001b[0m acc_per_fold\u001b[39m.\u001b[39mappend(model\u001b[39m.\u001b[39mevaluate(image_dataset[test], mask_dataset[test])[\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Noah\\.conda\\envs\\DeepACSA5\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1100\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1094\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1095\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1096\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1097\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1098\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1099\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1100\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1101\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1102\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Noah\\.conda\\envs\\DeepACSA5\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    827\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name) \u001b[39mas\u001b[39;00m tm:\n\u001b[1;32m--> 828\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    829\u001b[0m   compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experimental_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m   new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[1;32mc:\\Users\\Noah\\.conda\\envs\\DeepACSA5\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:855\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    852\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    853\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    854\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 855\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    857\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    858\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    859\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Noah\\.conda\\envs\\DeepACSA5\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2942\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2940\u001b[0m   (graph_function,\n\u001b[0;32m   2941\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2943\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Noah\\.conda\\envs\\DeepACSA5\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1918\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1914\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1915\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1916\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1917\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1918\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1919\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1920\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m     args,\n\u001b[0;32m   1922\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1923\u001b[0m     executing_eagerly)\n\u001b[0;32m   1924\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Noah\\.conda\\envs\\DeepACSA5\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:555\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    554\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 555\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    556\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    557\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    558\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    559\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    560\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    561\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    562\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    563\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    564\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    567\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    568\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Noah\\.conda\\envs\\DeepACSA5\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "architecture = \"Swin_unet\"\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(f'B1VL-Kfoldno{fold_no}-{architecture}.h5', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f'B1VL-Kfoldno{fold_no}-{architecture}.csv', separator=',', append=False)]\n",
    "  \n",
    "  #define the model architecture\n",
    "  #this model requires depth >= 2\n",
    "  model = models.swin_unet_2d((256, 256, 3), filter_num_begin=64, n_labels=num_labels, depth=4, stack_num_down=2, stack_num_up=2, \n",
    "                            patch_size=(4, 4), num_heads=[4, 8, 8, 8], window_size=[4, 2, 2, 2], num_mlp=512, \n",
    "                            output_activation='Softmax', shift_window=True, name='swin_unet') #Guess: Shift_window = False\n",
    "\n",
    "  #compile the model\n",
    "  model.compile(loss='binary_crossentropy', optimizer=Adam(lr = 1e-3), \n",
    "              metrics=['accuracy', IoU])\n",
    "\n",
    "  #generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  #fit model on data\n",
    "  swin_unet_history = model.fit(image_dataset[train], mask_dataset[train], \n",
    "                    verbose=1,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data=(image_dataset[test], mask_dataset[test]), \n",
    "                    shuffle=False,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "  #append evaluation values for every fold to a list\n",
    "  acc_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[1])\n",
    "  loss_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[0])\n",
    "  IoU_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[2])\n",
    "\n",
    "  #increase fold number\n",
    "  fold_no += 1\n",
    "\n",
    "#determine best fold\n",
    "best_fold(architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 with DL-Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of aponeurosis images =  512\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (256, 256, 1, 1) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 403\u001b[0m\n\u001b[0;32m    400\u001b[0m mask_dataset \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(mask_dataset)\n\u001b[0;32m    401\u001b[0m mask_dataset \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(mask_dataset, axis\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m--> 403\u001b[0m plt\u001b[39m.\u001b[39;49mimshow(mask_dataset[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m    404\u001b[0m plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m    405\u001b[0m plt\u001b[39m.\u001b[39mimshow(image_dataset[\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Noah\\.conda\\envs\\DeepACSA5\\lib\\site-packages\\matplotlib\\pyplot.py:2724\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2718\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n\u001b[0;32m   2719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n\u001b[0;32m   2720\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2721\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m,\n\u001b[0;32m   2722\u001b[0m         filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m, resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2723\u001b[0m         data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2724\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39;49mimshow(\n\u001b[0;32m   2725\u001b[0m         X, cmap\u001b[39m=\u001b[39;49mcmap, norm\u001b[39m=\u001b[39;49mnorm, aspect\u001b[39m=\u001b[39;49maspect,\n\u001b[0;32m   2726\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation, alpha\u001b[39m=\u001b[39;49malpha, vmin\u001b[39m=\u001b[39;49mvmin,\n\u001b[0;32m   2727\u001b[0m         vmax\u001b[39m=\u001b[39;49mvmax, origin\u001b[39m=\u001b[39;49morigin, extent\u001b[39m=\u001b[39;49mextent,\n\u001b[0;32m   2728\u001b[0m         filternorm\u001b[39m=\u001b[39;49mfilternorm, filterrad\u001b[39m=\u001b[39;49mfilterrad, resample\u001b[39m=\u001b[39;49mresample,\n\u001b[0;32m   2729\u001b[0m         url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}),\n\u001b[0;32m   2730\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   2731\u001b[0m     sci(__ret)\n\u001b[0;32m   2732\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32mc:\\Users\\Noah\\.conda\\envs\\DeepACSA5\\lib\\site-packages\\matplotlib\\__init__.py:1447\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1444\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m   1445\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1446\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1447\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1449\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1450\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[0;32m   1451\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mc:\\Users\\Noah\\.conda\\envs\\DeepACSA5\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5523\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5518\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m   5519\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap, norm, interpolation, origin, extent,\n\u001b[0;32m   5520\u001b[0m                       filternorm\u001b[39m=\u001b[39mfilternorm, filterrad\u001b[39m=\u001b[39mfilterrad,\n\u001b[0;32m   5521\u001b[0m                       resample\u001b[39m=\u001b[39mresample, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 5523\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[0;32m   5524\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5525\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5526\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Noah\\.conda\\envs\\DeepACSA5\\lib\\site-packages\\matplotlib\\image.py:711\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    707\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n\u001b[0;32m    709\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m    710\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m]):\n\u001b[1;32m--> 711\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m for image data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    712\u001b[0m                     \u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape))\n\u001b[0;32m    714\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m    715\u001b[0m     \u001b[39m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[0;32m    716\u001b[0m     \u001b[39m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[0;32m    717\u001b[0m     \u001b[39m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[0;32m    718\u001b[0m     \u001b[39m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[0;32m    719\u001b[0m     high \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger) \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (256, 256, 1, 1) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from skimage.io import imshow\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn import datasets\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.backend import clear_session\n",
    "from keras import backend as K\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
    "from tensorflow.keras.applications import VGG16 \n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "def conv_block(inputs, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding = \"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    x = Conv2D(num_filters, 3, padding = \"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def decoder_block(inputs, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs) #32\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def build_vgg16_unet(input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    vgg16 = VGG16(include_top=False, weights=\"imagenet\", input_tensor = inputs)\n",
    "    #vgg16.summary()\n",
    "    \n",
    "    \"\"\" Encoder \"\"\"\n",
    "    \n",
    "    # skip connections\n",
    "    s1 = vgg16.get_layer(\"block1_conv2\").output # 256\n",
    "    s2 = vgg16.get_layer(\"block2_conv2\").output # 128\n",
    "    s3 = vgg16.get_layer(\"block3_conv3\").output # 64\n",
    "    s4 = vgg16.get_layer(\"block4_conv3\").output # 32\n",
    "\n",
    "    \"\"\" Bottleneck/Bridge \"\"\"\n",
    "    \n",
    "    b1 = vgg16.get_layer(\"block5_conv3\").output # 16\n",
    "    \n",
    "    \"\"\" Decoder \"\"\"\n",
    "\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "    \n",
    "    \"\"\" Outputs \"\"\"\n",
    "    outputs = Conv2D(1, (1, 1), padding = \"same\", activation=\"sigmoid\")(d4) #binary segmentation\n",
    "    model = Model(inputs, outputs, name = \"VGG16_U-Net\")\n",
    "    return model\n",
    "    \n",
    "# Convolution block\n",
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size), \\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Create u-net model\n",
    "def get_unet(input_img, n_filters = 64, dropout = 0.1, batchnorm = True):\n",
    "    \"\"\"Function to define the UNET Model\"\"\"\n",
    "    \n",
    "    # Contracting Path\n",
    "    # c is output tensor of conv layers\n",
    "    # p ist output tensor of max pool layers\n",
    "    # u is output tensor of up-sampling (transposed) layers\n",
    "    # Batchnorm standardizes/normalizes the output of each layer where applied in order to avoid huge weights using \n",
    "    # z-scores \n",
    "    \n",
    "    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    p1 = Dropout(dropout)(p1)\n",
    "    \n",
    "    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "    \n",
    "    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "    \n",
    "    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "    \n",
    "    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    # Expansive Path\n",
    "    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# Compute Intersection over union (IoU), a measure of labelling accuracy\n",
    "# NOTE: This is sometimes also called Jaccard score\n",
    "def IoU(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    union = K.sum(y_true,-1) + K.sum(y_pred,-1) - intersection\n",
    "    iou = (intersection + smooth) / ( union + smooth)\n",
    "    return iou\n",
    "\n",
    "def dice_score(y_true, y_pred, smooth=1):\n",
    "    \n",
    "    # Flatten\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    \n",
    "    return 1 - dice_score(y_true, y_pred)\n",
    "\n",
    "def dice_bce_score(y_true, y_pred, smooth=1):    \n",
    "    \n",
    "    BCE =  K.binary_crossentropy(y_true, y_pred)\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)    \n",
    "    dice_loss = 1 - (2*intersection + smooth) / (K.sum(y_true, -1) + K.sum(y_pred, -1) + smooth)\n",
    "    Dice_BCE = BCE + dice_loss\n",
    "    \n",
    "    return Dice_BCE\n",
    "\n",
    "def focal_loss(y_true, y_pred, alpha=0.8, gamma=2):    \n",
    "      \n",
    "    BCE = K.binary_crossentropy(y_true, y_pred)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "    return focal_loss\n",
    "\n",
    "# Plot sample of model prediction\n",
    "def plot_sample(X, y, preds, binary_preds, ix=None):\n",
    "    if ix is None:\n",
    "        ix = random.randint(0, len(X))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(30, 20))\n",
    "    ax[0].imshow(X[ix, ..., 0], cmap='Greys_r')\n",
    "    \n",
    "    ax[0].set_title('US-image', c=\"white\" )\n",
    "    ax[0].grid(False)\n",
    "\n",
    "    ax[1].imshow(y[ix].squeeze(), cmap='Greys_r')\n",
    "    ax[1].set_title('Aponeurosis', c=\"white\")\n",
    "    ax[1].grid(False)\n",
    "\n",
    "    ax[2].imshow(preds[ix].squeeze(), vmin=0, vmax=1, cmap=\"Greys_r\")\n",
    "    \n",
    "    ax[2].set_title('Apo-Predicted', c=\"white\")\n",
    "    ax[2].grid(False)\n",
    "    \n",
    "    ax[3].imshow(binary_preds[ix].squeeze(), vmin=0, vmax=0.5, cmap=\"Greys_r\")\n",
    "    \n",
    "    ax[3].set_title('Apo-Picture binary', c=\"white\")\n",
    "    ax[3].grid(False)\n",
    "    \n",
    "    plt.savefig(str(ix)+\"Pred_area.tif\")\n",
    "\n",
    "# Save all predictions on disk \n",
    "def save_pred_area(binary_preds): \n",
    "    for i in range(len(binary_preds)): \n",
    "        fig, (ax1)= plt.subplots(1, 1, figsize = (15, 15))\n",
    "        ax1.imshow(binary_preds[i], cmap=\"Greys_r\", interpolation=\"bilinear\")\n",
    "        ax1.set_title(\"Predicted Area\")\n",
    "        plt.savefig(str(i)+\"Pred_area.tif\") # Saves images to directory of notebook\n",
    "\n",
    "# Convolution block\n",
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3, batchnorm = True):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size), \\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same')(input_tensor)\n",
    "    if batchnorm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Create u-net model\n",
    "def get_unet(input_img, n_filters = 64, dropout = 0.1, batchnorm = True):\n",
    "    \"\"\"Function to define the UNET Model\"\"\"\n",
    "    \n",
    "    # Contracting Path\n",
    "    # c is output tensor of conv layers\n",
    "    # p ist output tensor of max pool layers\n",
    "    # u is output tensor of up-sampling (transposed) layers\n",
    "    # Batchnorm standardizes/normalizes the output of each layer where applied in order to avoid huge weights using \n",
    "    # z-scores \n",
    "    \n",
    "    c1 = conv2d_block(input_img, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    p1 = Dropout(dropout)(p1)\n",
    "    \n",
    "    c2 = conv2d_block(p1, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "    p2 = Dropout(dropout)(p2)\n",
    "    \n",
    "    c3 = conv2d_block(p2, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "    p3 = Dropout(dropout)(p3)\n",
    "    \n",
    "    c4 = conv2d_block(p3, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    p4 = MaxPooling2D((2, 2))(c4)\n",
    "    p4 = Dropout(dropout)(p4)\n",
    "    \n",
    "    c5 = conv2d_block(p4, n_filters = n_filters * 16, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    # Expansive Path\n",
    "    u6 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    u6 = Dropout(dropout)(u6)\n",
    "    c6 = conv2d_block(u6, n_filters * 8, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u7 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    u7 = Dropout(dropout)(u7)\n",
    "    c7 = conv2d_block(u7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u8 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    u8 = Dropout(dropout)(u8)\n",
    "    c8 = conv2d_block(u8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    u9 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(c8)\n",
    "    u9 = concatenate([u9, c1])\n",
    "    u9 = Dropout(dropout)(u9)\n",
    "    c9 = conv2d_block(u9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm)\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "    model = Model(inputs=[input_img], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# Compute Intersection over union (IoU), a measure of labelling accuracy\n",
    "# NOTE: This is sometimes also called Jaccard score\n",
    "def IoU(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    union = K.sum(y_true,-1) + K.sum(y_pred,-1) - intersection\n",
    "    iou = (intersection + smooth) / ( union + smooth)\n",
    "    return iou\n",
    "\n",
    "def dice_score(y_true, y_pred, smooth=1):\n",
    "    \n",
    "    # Flatten\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    \n",
    "    return 1 - dice_score(y_true, y_pred)\n",
    "\n",
    "def dice_bce_score(y_true, y_pred, smooth=1):    \n",
    "    \n",
    "    BCE =  K.binary_crossentropy(y_true, y_pred)\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)    \n",
    "    dice_loss = 1 - (2*intersection + smooth) / (K.sum(y_true, -1) + K.sum(y_pred, -1) + smooth)\n",
    "    Dice_BCE = BCE + dice_loss\n",
    "    \n",
    "    return Dice_BCE\n",
    "\n",
    "def focal_loss(y_true, y_pred, alpha=0.8, gamma=2):    \n",
    "      \n",
    "    BCE = K.binary_crossentropy(y_true, y_pred)\n",
    "    BCE_EXP = K.exp(-BCE)\n",
    "    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n",
    "    return focal_loss\n",
    "\n",
    "# Plot sample of model prediction\n",
    "def plot_sample(X, y, preds, binary_preds, ix=None):\n",
    "    if ix is None:\n",
    "        ix = random.randint(0, len(X))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(30, 20))\n",
    "    ax[0].imshow(X[ix, ..., 0], cmap='Greys_r')\n",
    "    \n",
    "    ax[0].set_title('US-image', c=\"white\" )\n",
    "    ax[0].grid(False)\n",
    "\n",
    "    ax[1].imshow(y[ix].squeeze(), cmap='Greys_r')\n",
    "    ax[1].set_title('Aponeurosis', c=\"white\")\n",
    "    ax[1].grid(False)\n",
    "\n",
    "    ax[2].imshow(preds[ix].squeeze(), vmin=0, vmax=1, cmap=\"Greys_r\")\n",
    "    \n",
    "    ax[2].set_title('Apo-Predicted', c=\"white\")\n",
    "    ax[2].grid(False)\n",
    "    \n",
    "    ax[3].imshow(binary_preds[ix].squeeze(), vmin=0, vmax=0.5, cmap=\"Greys_r\")\n",
    "    \n",
    "    ax[3].set_title('Apo-Picture binary', c=\"white\")\n",
    "    ax[3].grid(False)\n",
    "    \n",
    "    plt.savefig(str(ix)+\"Pred_area.tif\")\n",
    "\n",
    "# Save all predictions on disk \n",
    "def save_pred_area(binary_preds): \n",
    "    for i in range(len(binary_preds)): \n",
    "        fig, (ax1)= plt.subplots(1, 1, figsize = (15, 15))\n",
    "        ax1.imshow(binary_preds[i], cmap=\"Greys_r\", interpolation=\"bilinear\")\n",
    "        ax1.set_title(\"Predicted Area\")\n",
    "        plt.savefig(str(i)+\"Pred_area.tif\") # Saves images to directory of notebook\n",
    "    \n",
    "# Images will be re-scaled\n",
    "im_width = 256\n",
    "im_height = 256\n",
    "border = 5\n",
    "\n",
    "# list of all images in the path\n",
    "ids = os.listdir('D:/UniBas/Bachelorarbeit/Img_masks/DeepACSA_images_RF/insert_images/')\n",
    "print(\"Total no. of aponeurosis images = \", len(ids))\n",
    "X = np.zeros((len(ids), im_height, im_width, 1), dtype=np.float32)\n",
    "y = np.zeros((len(ids), im_height, im_width, 1), dtype=np.float32)\n",
    "\n",
    "image_dataset = []\n",
    "for path in glob.glob('D:/UniBas/Bachelorarbeit/Img_masks/DeepACSA_images_RF/insert_images/'):\n",
    "    for img_path in glob.glob(os.path.join(path, \"*.tif\")):\n",
    "        img = cv2.imread(img_path, 1)\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        img = img_to_array(img)\n",
    "        img = img/255.0\n",
    "        image_dataset.append(img)  \n",
    "image_dataset = np.array(image_dataset)\n",
    "\n",
    "mask_dataset = []\n",
    "for path in glob.glob('D:/UniBas/Bachelorarbeit/Img_masks/DeepACSA_masks_RF/insert_masks/'):\n",
    "    for mask_path in glob.glob(os.path.join(path, \"*.tif\")):\n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "        mask = cv2.resize(mask, (256,256))\n",
    "        mask = img_to_array(mask)\n",
    "        mask = mask/255.0\n",
    "        mask_dataset.append(mask)\n",
    "        \n",
    "mask_dataset = np.array(mask_dataset)\n",
    "mask_dataset = np.expand_dims(mask_dataset, axis=3)\n",
    "\n",
    "###################################################################################\n",
    "batch_size = 2\n",
    "epochs = 60\n",
    "num_folds = 5\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "\n",
    "# Define per-fold score containers \n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "#compile the model\n",
    "VGG16_UNet = build_vgg16_unet((256,256,3)) #input_shape is (256, 256, 3)\n",
    "model_apo = VGG16_UNet\n",
    "model_apo.compile(optimizer=Adam(), loss=dice_bce_score, metrics=[\"accuracy\", IoU])\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "architecture = \"VGG16\"\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "\n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(f'B1VL-Kfoldno{fold_no}-{architecture}.h5', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f'B1VL-Kfoldno{fold_no}-{architecture}.csv', separator=',', append=False)\n",
    "  ]\n",
    "\n",
    "  # Generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  # Fit data to model\n",
    "  results = model_apo.fit(image_dataset[train], mask_dataset[train], batch_size=batch_size, epochs=epochs,\n",
    "                       callbacks=callbacks, validation_data=(image_dataset[test], mask_dataset[test]))\n",
    "\n",
    "  # Increase fold number\n",
    "  fold_no = fold_no + 1\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "#determine best fold\n",
    "best_fold(architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with ResUnet (not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = \"ResUnet\"\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(f'B1VL-Kfoldno{fold_no}-{architecture}.h5', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f'B1VL-Kfoldno{fold_no}-{architecture}.csv', separator=',', append=False)]\n",
    "\n",
    "  # Define the model architecture\n",
    "  # resunet requires depth >= 2\n",
    "  resunet = models.resunet_a_2d((256, 256, 3), [64, 128, 256, 512], \n",
    "                            dilation_num=[1, 3, 15, 31], \n",
    "                            n_labels=2, aspp_num_down=256, aspp_num_up=128, \n",
    "                            activation='ReLU', output_activation='Softmax', \n",
    "                            batch_norm=True, pool=\"max\", unpool='nearest', name='resunet')\n",
    "\n",
    "  # Compile the model\n",
    "  resunet.compile(loss='binary_crossentropy', optimizer=Adam(lr = 1e-3), \n",
    "              metrics=['accuracy', IoU])\n",
    "\n",
    "  # Generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  # Fit data to model\n",
    "  resunet_history = resunet.fit(image_dataset[train], mask_dataset[train], \n",
    "                    verbose=1,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data=(image_dataset[test], mask_dataset[test]), \n",
    "                    shuffle=False,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "  #append evaluation values for every fold to a list\n",
    "  acc_per_fold.append(resunet.evaluate(image_dataset[test], mask_dataset[test])[1])\n",
    "  loss_per_fold.append(resunet.evaluate(image_dataset[test], mask_dataset[test])[0])\n",
    "  IoU_per_fold.append(resunet.evaluate(image_dataset[test], mask_dataset[test])[2])\n",
    "\n",
    "  # Increase fold number\n",
    "  fold_no += 1\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "#determine best fold\n",
    "best_fold(architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Kfold with r2_unet (not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = \"r2_unet\"\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(f'B1VL-Kfoldno{fold_no}-{architecture}.h5', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f'B1VL-Kfoldno{fold_no}-{architecture}.csv', separator=',', append=False)]                                  # Give the CSV file a name (.csv)\n",
    "\n",
    "  #define the model architecture\n",
    "  #r2_unet_2d requires depth >= 2\n",
    "  model = models.r2_unet_2d((256, 256, 3), [64, 128, 256, 512], n_labels=num_labels,\n",
    "                          stack_num_down=2, stack_num_up=2, recur_num=2,\n",
    "                          activation='ReLU', output_activation='Softmax', \n",
    "                          batch_norm=True, pool='max', unpool='nearest', name='r2unet')\n",
    "\n",
    "  #compile the model\n",
    "  model.compile(loss='binary_crossentropy', optimizer=Adam(lr = 1e-3), \n",
    "              metrics=['accuracy', IoU])\n",
    "\n",
    "  #generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  #fit model on data\n",
    "  resunet_history = model.fit(image_dataset[train], mask_dataset[train], \n",
    "                    verbose=1,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data=(image_dataset[test], mask_dataset[test]), \n",
    "                    shuffle=False,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "  #append evaluation values for every fold to a list\n",
    "  acc_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[1])\n",
    "  loss_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[0])\n",
    "  IoU_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[2])\n",
    "\n",
    "  #increase fold number\n",
    "  fold_no += 1\n",
    "\n",
    "fold_no = 1\n",
    "\n",
    "#determine best fold\n",
    "best_fold(architecture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide average scores for K-fold cross validation (outdated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]} - IoU: {IoU_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print(f'> IoU: {np.mean(IoU_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepACSA5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
