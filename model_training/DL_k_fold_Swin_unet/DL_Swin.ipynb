{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import random\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "#import glob\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use(\"ggplot\")\n",
    "#%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import cv2\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras_unet_collection import models #Keras unet collection\n",
    "from keras import utils\n",
    "#import tensorflow as tf\n",
    "from PIL import Image\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from skimage.io import imshow\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn import datasets\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.backend import clear_session\n",
    "from keras import backend as K\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
    "from tensorflow.keras.applications import VGG16 \n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Swin without k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, concatenate\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.utils' has no attribute 'shuffle_ind'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m label_names \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39msorted\u001b[39m(glob(filepath\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.tif\u001b[39m\u001b[39m'\u001b[39m)))\n\u001b[0;32m     26\u001b[0m L \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(sample_names)\n\u001b[1;32m---> 27\u001b[0m ind_all \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mshuffle_ind(L)\n\u001b[0;32m     29\u001b[0m L_train \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m0.8\u001b[39m\u001b[39m*\u001b[39mL); L_valid \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m0.1\u001b[39m\u001b[39m*\u001b[39mL); L_test \u001b[39m=\u001b[39m L \u001b[39m-\u001b[39m L_train \u001b[39m-\u001b[39m L_valid\n\u001b[0;32m     30\u001b[0m ind_train \u001b[39m=\u001b[39m ind_all[:L_train]; ind_valid \u001b[39m=\u001b[39m ind_all[L_train:L_train\u001b[39m+\u001b[39mL_valid]; ind_test \u001b[39m=\u001b[39m ind_all[L_train\u001b[39m+\u001b[39mL_valid:]\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.keras.utils' has no attribute 'shuffle_ind'"
     ]
    }
   ],
   "source": [
    "# the indicator of a fresh run\n",
    "first_time_running = True\n",
    "\n",
    "# user-specified working directory\n",
    "filepath = 'D:/UniBas/Bachelorarbeit/'\n",
    "\n",
    "if first_time_running:\n",
    "    # downloading and executing data files\n",
    "    import tarfile\n",
    "    import urllib.request\n",
    "    \n",
    "    filename_image = filepath+'some_images/'\n",
    "    filename_target = filepath+'some_masks/'\n",
    "\n",
    "def input_data_process(input_array):\n",
    "    '''converting pixel vales to [0, 1]'''\n",
    "    return input_array/255.\n",
    "\n",
    "def target_data_process(target_array):\n",
    "    '''Converting tri-mask of {1, 2, 3} to three categories.'''\n",
    "    return keras.utils.to_categorical(target_array-1)\n",
    "\n",
    "sample_names = np.array(sorted(glob(filepath+'.tif')))\n",
    "label_names = np.array(sorted(glob(filepath+'.tif')))\n",
    "\n",
    "L = len(sample_names)\n",
    "ind_all = utils.shuffle_ind(L)\n",
    "\n",
    "L_train = int(0.8*L); L_valid = int(0.1*L); L_test = L - L_train - L_valid\n",
    "ind_train = ind_all[:L_train]; ind_valid = ind_all[L_train:L_train+L_valid]; ind_test = ind_all[L_train+L_valid:]\n",
    "print(\"Training:validation:testing = {}:{}:{}\".format(L_train, L_valid, L_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n",
      "(256, 256, 2)\n",
      "Epoch 1/2\n",
      "230/230 [==============================] - 365s 2s/step - loss: 0.1966 - accuracy: 0.9209 - IoU: 0.9253 - val_loss: 0.0586 - val_accuracy: 0.9774 - val_IoU: 0.9667\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05858, saving model to A_modelSwinV1.h5\n",
      "Epoch 2/2\n",
      "230/230 [==============================] - 338s 1s/step - loss: 0.0653 - accuracy: 0.9736 - IoU: 0.9694 - val_loss: 0.0498 - val_accuracy: 0.9800 - val_IoU: 0.9740\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05858 to 0.04975, saving model to A_modelSwinV1.h5\n",
      "R2-Unet fitting time is:  0:11:59.499520\n"
     ]
    }
   ],
   "source": [
    "#define directory where images and masks are located on local disk\n",
    "image_directory = 'D:/UniBas/Bachelorarbeit/Img_masks/DeepACSA_images_RF/insert_images/'\n",
    "mask_directory = 'D:/UniBas/Bachelorarbeit/Img_masks/DeepACSA_masks_RF/insert_masks/'\n",
    "#'D:/UniBas/Bachelorarbeit/Img_masks/DeepACSA_masks_RF/insert_masks/'\n",
    "\n",
    "#define the properties and empty list for resized images and masks\n",
    "SIZE = 256\n",
    "image_dataset = []\n",
    "mask_dataset = []\n",
    "\n",
    "\n",
    "#define custom functions\n",
    "def IoU(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    union = K.sum(y_true,-1) + K.sum(y_pred,-1) - intersection\n",
    "    iou = (intersection + smooth) / ( union + smooth)\n",
    "    return iou\n",
    "\n",
    "#enumerate and resize images/masks\n",
    "images = os.listdir(image_directory)\n",
    "for i, image_name in enumerate(images):    #enumerate method adds a counter and returns the enumerate object\n",
    "    if (image_name.split('.')[1] == 'tif'):\n",
    "        #print(image_directory+image_name)\n",
    "        image = cv2.imread(image_directory+image_name, 1)\n",
    "        image = Image.fromarray(image)\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        image_dataset.append(np.array(image))\n",
    "\n",
    "\n",
    "masks = os.listdir(mask_directory)\n",
    "for i, image_name in enumerate(masks):\n",
    "    if (image_name.split('.')[1] == 'tif'):\n",
    "        image = cv2.imread(mask_directory+image_name, 0)\n",
    "        image = Image.fromarray(image)\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        mask_dataset.append(np.array(image))\n",
    "\n",
    "\n",
    "#define some hyperparameters\n",
    "num_labels = 2  #Binary classificaion (missmatch with literature/code examples!)\n",
    "batch_size = 2  #keep it smaller than 3\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "#normalize images\n",
    "image_dataset = np.array(image_dataset)/255\n",
    "#do not normalize masks, just rescale to 0 to 1. Add RGB-Chanel (3) to mask.\n",
    "mask_dataset = np.expand_dims((np.array(mask_dataset)),3) /255\n",
    "mask_dataset = tf.keras.utils.to_categorical(mask_dataset)\n",
    "\n",
    "\"\"\" plt.imshow(mask_dataset[1])\n",
    "plt.show()\n",
    "plt.imshow(image_dataset[1])\n",
    "plt.show() \"\"\"\n",
    "\n",
    "print(image_dataset[1].shape)\n",
    "print(mask_dataset[1].shape)\n",
    "\n",
    "#define callback function\n",
    "callbacks = [\n",
    "  EarlyStopping(patience=8, verbose=1),\n",
    "  ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "  ModelCheckpoint('A_modelSwinV1.h5', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "  CSVLogger('A_modelSwinV1.csv', separator=',', append=False)]\n",
    "    \n",
    "#define the model architecture\n",
    "#this model requires depth >= 2\n",
    "model = models.swin_unet_2d((256, 256, 3), filter_num_begin=64, n_labels=num_labels, depth=4, stack_num_down=2, stack_num_up=2, \n",
    "                            patch_size=(4, 4), num_heads=[4, 8, 8, 8], window_size=[4, 2, 2, 2], num_mlp=512, \n",
    "                            output_activation='Softmax', shift_window=True, name='swin_unet') #Guess: Shift_window = False\n",
    "\n",
    "#compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr = 1e-3), \n",
    "            metrics=['accuracy', IoU])\n",
    "\n",
    "#split dataset into training and validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_dataset, mask_dataset, test_size = 0.10, random_state = 0)\n",
    "\n",
    "#start time meauserment\n",
    "start2 = datetime.now() \n",
    "\n",
    "#fit model to data\n",
    "resunet_history = model.fit(X_train, y_train, \n",
    "                    verbose=1,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data=(X_test, y_test), \n",
    "                    shuffle=False,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "#stop time measurement and print the execution time\n",
    "stop2 = datetime.now() \n",
    "execution_time = stop2-start2\n",
    "print(\"R2-Unet fitting time is: \", execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (256, 256, 1, 1) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m mask_dataset \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(mask_dataset)\n\u001b[0;32m     21\u001b[0m mask_dataset \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(mask_dataset, axis\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m plt\u001b[39m.\u001b[39;49mimshow(mask_dataset[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m     24\u001b[0m plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m     25\u001b[0m plt\u001b[39m.\u001b[39mimshow(image_dataset[\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Noah\\.conda\\envs\\DeepACSA5\\lib\\site-packages\\matplotlib\\pyplot.py:2724\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2718\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n\u001b[0;32m   2719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n\u001b[0;32m   2720\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2721\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m,\n\u001b[0;32m   2722\u001b[0m         filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m, resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2723\u001b[0m         data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2724\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39;49mimshow(\n\u001b[0;32m   2725\u001b[0m         X, cmap\u001b[39m=\u001b[39;49mcmap, norm\u001b[39m=\u001b[39;49mnorm, aspect\u001b[39m=\u001b[39;49maspect,\n\u001b[0;32m   2726\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation, alpha\u001b[39m=\u001b[39;49malpha, vmin\u001b[39m=\u001b[39;49mvmin,\n\u001b[0;32m   2727\u001b[0m         vmax\u001b[39m=\u001b[39;49mvmax, origin\u001b[39m=\u001b[39;49morigin, extent\u001b[39m=\u001b[39;49mextent,\n\u001b[0;32m   2728\u001b[0m         filternorm\u001b[39m=\u001b[39;49mfilternorm, filterrad\u001b[39m=\u001b[39;49mfilterrad, resample\u001b[39m=\u001b[39;49mresample,\n\u001b[0;32m   2729\u001b[0m         url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}),\n\u001b[0;32m   2730\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   2731\u001b[0m     sci(__ret)\n\u001b[0;32m   2732\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32mc:\\Users\\Noah\\.conda\\envs\\DeepACSA5\\lib\\site-packages\\matplotlib\\__init__.py:1447\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1444\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m   1445\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1446\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1447\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1449\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1450\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[0;32m   1451\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mc:\\Users\\Noah\\.conda\\envs\\DeepACSA5\\lib\\site-packages\\matplotlib\\axes\\_axes.py:5523\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5518\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m   5519\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap, norm, interpolation, origin, extent,\n\u001b[0;32m   5520\u001b[0m                       filternorm\u001b[39m=\u001b[39mfilternorm, filterrad\u001b[39m=\u001b[39mfilterrad,\n\u001b[0;32m   5521\u001b[0m                       resample\u001b[39m=\u001b[39mresample, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 5523\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[0;32m   5524\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5525\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5526\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Noah\\.conda\\envs\\DeepACSA5\\lib\\site-packages\\matplotlib\\image.py:711\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    707\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n\u001b[0;32m    709\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m    710\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m]):\n\u001b[1;32m--> 711\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m for image data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    712\u001b[0m                     \u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape))\n\u001b[0;32m    714\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m    715\u001b[0m     \u001b[39m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[0;32m    716\u001b[0m     \u001b[39m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[0;32m    717\u001b[0m     \u001b[39m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[0;32m    718\u001b[0m     \u001b[39m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[0;32m    719\u001b[0m     high \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger) \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (256, 256, 1, 1) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_dataset = []\n",
    "for path in glob.glob('D:/UniBas/Bachelorarbeit/Img_masks/DeepACSA_images_RF/insert_images/'):\n",
    "    for img_path in glob.glob(os.path.join(path, \"*.tif\")):\n",
    "        img = cv2.imread(img_path, 1)\n",
    "        img = cv2.resize(img, (256,256))\n",
    "        img = img_to_array(img)\n",
    "        img = img/255.0\n",
    "        image_dataset.append(img)  \n",
    "image_dataset = np.array(image_dataset)\n",
    "\n",
    "mask_dataset = []\n",
    "for path in glob.glob('D:/UniBas/Bachelorarbeit/Img_masks/DeepACSA_masks_RF/insert_masks/'):\n",
    "    for mask_path in glob.glob(os.path.join(path, \"*.tif\")):\n",
    "        mask = cv2.imread(mask_path, 0)\n",
    "        mask = cv2.resize(mask, (256,256))\n",
    "        mask = img_to_array(mask)\n",
    "        mask = mask/255.0\n",
    "        mask_dataset.append(mask)\n",
    "        \n",
    "mask_dataset = np.array(mask_dataset)\n",
    "mask_dataset = np.expand_dims(mask_dataset, axis=3)\n",
    "\n",
    "plt.imshow(mask_dataset[1])\n",
    "plt.show()\n",
    "plt.imshow(image_dataset[1])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Swin with k-Fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "16/16 [==============================] - 56s 3s/step - loss: 1.1253 - accuracy: 0.4944 - IoU: 0.5445 - val_loss: 0.7210 - val_accuracy: 0.4984 - val_IoU: 0.5465\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.72103, saving model to K-foldno1-swinunet-V1-VL-256.h5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.7210 - accuracy: 0.4984 - IoU: 0.5465\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.7210 - accuracy: 0.4984 - IoU: 0.5465\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.7210 - accuracy: 0.4984 - IoU: 0.5465\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "16/16 [==============================] - 62s 3s/step - loss: 1.0039 - accuracy: 0.5070 - IoU: 0.5480 - val_loss: 0.7046 - val_accuracy: 0.5332 - val_IoU: 0.5431\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70462, saving model to K-foldno2-swinunet-V1-VL-256.h5\n",
      "1/1 [==============================] - 10s 10s/step - loss: 0.7046 - accuracy: 0.5332 - IoU: 0.5431\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.7046 - accuracy: 0.5332 - IoU: 0.5431\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.7046 - accuracy: 0.5332 - IoU: 0.5431\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.7210310697555542 - Accuracy: 0.49837589263916016%\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.7046186923980713 - Accuracy: 0.533228874206543%\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> Accuracy: 0.5158023834228516 (+- 0.017426490783691406)\n",
      "> Loss: 0.7128248810768127\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#define directory where images and masks are located on local disk\n",
    "image_directory = 'D:/UniBas/Bachelorarbeit/Img_masks/DeepACSA_images_RF/insert_images'\n",
    "mask_directory = 'D:/UniBas/Bachelorarbeit/Img_masks/DeepACSA_images_RF/insert_masks/'\n",
    "\n",
    "#define the properties and empty list for resized images and masks\n",
    "SIZE = 256\n",
    "image_dataset = []\n",
    "mask_dataset = []\n",
    "\n",
    "#define custom functions\n",
    "def IoU(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    union = K.sum(y_true,-1) + K.sum(y_pred,-1) - intersection\n",
    "    iou = (intersection + smooth) / ( union + smooth)\n",
    "    return iou\n",
    "\n",
    "#enumerate and resize images/masks\n",
    "images = os.listdir(image_directory)\n",
    "for i, image_name in enumerate(images):    #enumerate method adds a counter and returns the enumerate object\n",
    "    if (image_name.split('.')[1] == 'tif'):\n",
    "        #print(image_directory+image_name)\n",
    "        image = cv2.imread(image_directory+image_name, 1)\n",
    "        image = Image.fromarray(image)\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        image_dataset.append(np.array(image))\n",
    "\n",
    "masks = os.listdir(mask_directory)\n",
    "for i, image_name in enumerate(masks):\n",
    "    if (image_name.split('.')[1] == 'tif'):\n",
    "        image = cv2.imread(mask_directory+image_name, 0)\n",
    "        image = Image.fromarray(image)\n",
    "        image = image.resize((SIZE, SIZE))\n",
    "        mask_dataset.append(np.array(image))\n",
    "\n",
    "#define some hyperparameters\n",
    "num_labels = 1  #Binary classificaion (missmatch on coding examples! 1 or 2 for binary classification)\n",
    "batch_size = 2  #keep it smaller than 3\n",
    "epochs = 60\n",
    "num_folds = 5   #define the number of folds (usually 5-10 folds)\n",
    "\n",
    "#normalize images\n",
    "image_dataset = np.array(image_dataset)/255.\n",
    "#do not normalize masks, just rescale to 0 to 1.\n",
    "mask_dataset = np.expand_dims((np.array(mask_dataset)),3) /255.\n",
    "\n",
    "#define K-fold Cross Validator\n",
    "kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "\n",
    "#define per-fold score containers \n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "IoU_per_fold = []\n",
    "\n",
    "#K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for train, test in kfold.split(image_dataset, mask_dataset):\n",
    "  callbacks = [\n",
    "    EarlyStopping(patience=8, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.00001, verbose=1),\n",
    "    ModelCheckpoint(f'K-foldno{fold_no}-swinunet-V1-VL-256.h5', verbose=1, save_best_only=True, save_weights_only=False), # Give the model a name (the .h5 part)\n",
    "    CSVLogger(f'K-foldno{fold_no}-swinunet-V1-VL-256.csv', separator=',', append=False)]\n",
    "  \n",
    "  #define the model architecture\n",
    "  #this model requires depth >= 2\n",
    "  model = models.swin_unet_2d((256, 256, 3), filter_num_begin=64, n_labels=num_labels, depth=4, stack_num_down=2, stack_num_up=2, \n",
    "                            patch_size=(2, 2), num_heads=[4, 8, 8, 8], window_size=[4, 2, 2, 2], num_mlp=512, \n",
    "                            output_activation='Softmax', shift_window=False, name='swin_unet') #Guess: Shift_window = False\n",
    "\n",
    "  #compile the model\n",
    "  model.compile(loss='binary_crossentropy', optimizer=Adam(lr = 1e-3), \n",
    "              metrics=['accuracy', IoU])\n",
    "\n",
    "  #generate a print\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "  #fit model on data\n",
    "  swin_unet_history = model.fit(image_dataset[train], mask_dataset[train], \n",
    "                    verbose=1,\n",
    "                    batch_size = batch_size,\n",
    "                    validation_data=(image_dataset[test], mask_dataset[test]), \n",
    "                    shuffle=False,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "  #append evaluation values for every fold to a list\n",
    "  acc_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[1])\n",
    "  loss_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[0])\n",
    "  IoU_per_fold.append(model.evaluate(image_dataset[test], mask_dataset[test])[2])\n",
    "\n",
    "  #increase fold number\n",
    "  fold_no += 1\n",
    "\n",
    "# == Provide average scores ==\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Score per fold')\n",
    "for i in range(0, len(acc_per_fold)):\n",
    "  print('------------------------------------------------------------------------')\n",
    "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]} - IoU: {IoU_per_fold[i]}%')\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "print(f'> IoU: {np.mean(IoU_per_fold)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepACSA5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9838a3f48d92ef1e13f85ce4f6b5d67652e8280cf6406f0fff9e9d21f80d92c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
